
<!DOCTYPE html>

    <html lang="zh-Hant-CN">

    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="雨天等放晴">
    <title>深度學習中一些常用的內建損失函数 - 雨天等放晴</title>
    <meta name="author" content="Tang Huan">
    
        <meta name="keywords" content="PyTorch,Deep Learning,Loss Functions,損失函數,损失函数">
    
    
        <link rel="icon" href="https://tangh.github.io/assets/images/favicon.png">
    
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://tangh.github.io/assets/images/apple-touch-icon.png">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Tang Huan","sameAs":["https://twitter.com/tanghrtx/","https://www.flickr.com/photos/135277712@N07/","https://www.instagram.com/tanghrtx/","https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/","https://space.bilibili.com/634428/"],"image":"icon.jpg"},"articleBody":"\n\n\n這是最近的第二篇，目前只寫了交叉熵，以後有時間會加上別的。\n交叉熵（CrossEntropy）\nnn.CrossEntropyLoss\nnn.BCELoss\nnn.BCEWithLogitsLoss\n\nn_classes &gt; 1這應該是與分類相關最常見的一個損失函数，在目標檢測中對每個框的類別分数，和語義分割中對每個像素點的類別置信度訓練時，都可以使用這個損失函数。語義分割中的每一個像素就等效於目標檢測中的一個框，和圖像分類中的一整張圖。\n在 PyTorch 中，交叉熵在 nn.CrossEntropyLoss，它就是將 nn.LogSoftmax 和 nn.NLLLoss 兩個合在一起。\n對於 LogSoftmax，也就是在 softmax 的值上求了一个對数：\n$$\\text{LogSoftmax}(x_i) = log(\\frac{exp(x_i)}{\\sum_j exp(x_j)})$$\n其中 j 是所有的類別，總數為 n_classes。\n但是使用 LogSoftmax 比分成兩步数值上會更穩定：由於指數的存在，當輸入值 xi 過小時，在 softmax 時可能向下溢出而变為 0（在 log 之後就會是無限大）；過大時，指数計算可能會向上溢出。而計算 softmax 的 log 時可對分子分母同時除以（最）大的指数值：\n$$\\begin{aligned}log(softmax(x_i)) &amp; = log(\\frac {\\frac{e^{x_i}}{e^M}} {\\frac{e^{x_1}}{e^M} + \\frac{e^{x_2}}{e^M} + \\cdots + \\frac{e^{x_n}}{e^M}})\\ = \\ log(\\frac{e^{x_i - M}}{\\sum_j e^{x_j - M}}) \\\\&amp; = log(e^{x_i - M}) - log(\\sum_j e^{x_j - M}) \\\\&amp; = (x_i - M) - log(\\sum_j e^{x_j - M})\\end{aligned}$$\n這樣就避免了上述問題，同時可以看到計算速度會加快不少。對輸出的每個值都（在類別通道上）完成上述計算之後，NLLLoss (negative log likelihood loss) 就是對每一個 box，取出其 GT 對應的通道上的 LogSoftmax(xi) 值，取相反数，最後所有取出的值相加。\nclass NLLLoss() 在構造時有三個參数，一個是 weight，表示對某一個 class，取出來值取相反数後，乘上一個權重；另一個是 ignore_index，表示真實標籤中需要忽略的值，一般圖像中邊界和無法辨別的區域會用一個負值標記；reduction=&quot;mean&quot; 表示對所有 dimension 求解之後，求和並除以 batch * prod(dimension)，而 &quot;sum&quot; 表示求和後直接輸出，&quot;none&quot; 表示直接輸出数組 shape = (batch, d1, ..., dk)。\n下面測試一下，假設是圖像分類 batch = 3, n_classes = 4，那麼 model output logits 應是一個 (3, 4) 的 tensor，而 GT 應該是 (3，) tensor（其他 task 只是有更多 dimension N C d1 ... dk）\n123456789101112131415161718192021prediction_logtis = torch.rand(3, 4)  # (batch, C)&gt;&gt;&gt; tensor(    [[0.2870, 0.2232, 0.6423, 0.6035],     [0.4180, 0.9434, 0.6034, 0.5521],     [0.0026, 0.4993, 0.7166, 0.2258]]    )log_sm = F.log_softmax(prediction_logtis, dim=-1)&gt;&gt;&gt; tensor(    [[-1.5554, -1.6192, -1.2001, -1.2389],     [-1.6171, -1.0916, -1.4317, -1.4830],     [-1.7810, -1.2842, -1.0669, -1.5578]]    )math.log(math.exp(0.2870) / (math.exp(0.2870)+math.exp(0.2232)+math.exp(0.6423)+math.exp(0.6035)))&gt;&gt;&gt; -1.5554288144704078target = torch.tensor([0, 2, 1])  # (batch, )nll_loss = torch.nn.NLLLoss(reduction=\"mean\")nll_loss(log_sm, target)&gt;&gt;&gt; tensor(1.4238)\n\n手動計算一下 loss：(1.5554 + 1.4317 + 1.2842) / 3 = 1.4238。\n這種是 label 為 long int 的形式，在 tensorflow 中為 v1: sparse_softmax_cross_entropy_with_logits v2: tf.keras.losses.SparseCategoricalCrossentropy\n1234567891011tf_pred = prediction_logtis.detach().numpy()tf_target = target.detach().numpy()# tfv1tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_target, logits=tf_pred)&gt;&gt;&gt; &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.5553646, 1.4316716, 1.2842305], dtype=float32)&gt;# tfv2scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)scce(tf_target, tf_pred)&gt;&gt;&gt; &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4237556&gt;\n\n此外 tf 還允許一種 one hot 形式的 label，v1: softmax_cross_entropy_with_logits_v2 v2: tf.keras.losses.CategoricalCrossentropy，例如上面的 label 可以改為\n1234567891011121314151617181920# torchtarget_onehot = [    [1, 0, 0, 0],    [0, 0, 1, 0],    [0, 1, 0, 0],]  # (minibatch, C)nll_loss_onehot = -(torch.tensor(target_onehot) * log_sm).sum() / prediction.shape[0]&gt;&gt;&gt; tensor(1.4238)# tfv1tf_nll_loss_onehot = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(target_onehot, tf_pred)&gt;&gt;&gt; &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([1.5553646, 1.4316716, 1.2842305], dtype=float32)&gt;tf.math.reduce_mean(tf_nll_loss_onehot)&gt;&gt;&gt; &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4237556&gt;# tfv2tf_cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)tf_cce(target_onehot, tf_pred)&gt;&gt;&gt; &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.4237556&gt;\n\n對於 one-hot 的形式，還可以改為使用 soft label，表示其概率上不獨立，即 label:0 -&gt; [1, 0, 0] -&gt; [0.9, 0.09, 0.01]，tensorflow 可以直接使用，pytorch 需要自定義一個，即上述手動的乘法。我發現 mmcls/soft_cross_entropy 有一個現成的自定義 CrossEntropyLoss 實現，和上面一回事。\n\n\n對於 nn.CrossEntropyLoss，如前所述，就是把兩步合在了一起，所以它的公式如下：\n$$\\text{CELoss} (x, \\text{class}) = - log(\\frac{e^{x[class]}}{\\sum_j e^{x[j]}}) = -x[class] + log(\\sum_j e^{x[j]})$$\n它的參数也與 NLLLoss 相同，這裡以一個 Batch Size = 2, classes = 3, size = (3, 3) 的輸出為例，即語義分割任務，看看兩者是不是相等的：\n123456789101112131415prediction = torch.rand(2, 3, 3, 3)# 語義分割 GT 需要 NHW，int64 格式，shape = (B, d1, d2)target = torch.tensor([    [[0,2,1],[0,2,1],[0,2,1]],    [[0,2,1],[0,2,1],[0,2,1]]])log_sm = F.log_softmax(prediction, dim=1)nlll = nn.NLLLoss(reduction=\"sum\")print(nlll(log_sm, target) / 18)&gt;&gt;&gt; tensor(1.1208)cel = nn.CrossEntropyLoss(reduction=\"mean\")print(cel(prediction, target))&gt;&gt;&gt; tensor(1.1208)\n\n可以看到二者結果完全相同。\nn_class = 1交叉熵還有一個二元（Binary）形式的 nn.BCELoss，它要求 GT 只能是 0 或 1 兩個值，比如 Faster R-CNN 中的 RPN 模塊的訓練就可以使用它，只有兩類後，公式簡化如下：\n$$l_n = -w_n [y_n \\cdot log(x_n) + (1-y_n) \\cdot log(1-x_n) ]$$\n可以使用 torch.nn.functional.binary_cross_entropy，與 sklearn.metrics.log_loss 完全相同。\nsigmoid/softmax in binary\nThe sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression.\n對於 sigmoid，最後輸出一個值，輸入是 x，參數是 θ，那麼 sigmoid 的輸出為\n$$p(y = 1|x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\np(y = 0|x) = 1 - p(y = 1|x)，所以\n$$p(y = 0|x) = \\frac{e^{-\\theta^Tx}}{1 + e^{-\\theta^Tx}}$$\n對於 softmax，最後的輸出是兩個值，輸入是 x，兩個輸出的參數分別是 θ_n，那麼 softmax 的輸出為\n$$p(y = 1|x) = \\frac{e^{\\theta_1^Tx}}{e^{\\theta_0^Tx} + e^{\\theta_1^Tx}} = \\frac{1}{1 + e^{(\\theta_0^T – \\theta_1^T)x}} = \\frac{1}{1 + e^{-\\beta x}}$$$$p(y = 0|x) = \\frac{e^{\\theta_0^Tx}}{e^{\\theta_0^Tx} + e^{\\theta_1^Tx}} = \\frac{e^{(\\theta_0^T-\\theta_1^T)x}}{1 + e^{(\\theta_0^T-\\theta_1^T)x}} = \\frac{e^{-\\beta x}}{1 + e^{-\\beta x}}$$\n其中 \\( \\beta = -(\\theta_0^T – \\theta_1^T) \\)，二分類的情況下，softmax 退化為 sigmoid。nn.BCEWithLogitsLoss 將 sigmoid 激活和 BCELoss 結合，同樣更數值穩定。\n","dateCreated":"2020-03-31T00:00:00+08:00","dateModified":"2022-03-18T00:45:35+08:00","datePublished":"2020-03-31T00:00:00+08:00","description":"一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。","headline":"深度學習中一些常用的內建損失函数","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"},"publisher":{"@type":"Organization","name":"Tang Huan","sameAs":["https://twitter.com/tanghrtx/","https://www.flickr.com/photos/135277712@N07/","https://www.instagram.com/tanghrtx/","https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/","https://space.bilibili.com/634428/"],"image":"icon.jpg","logo":{"@type":"ImageObject","url":"icon.jpg"}},"url":"https://tangh.github.io/articles/frequently-used-built-in-loss-functions/","keywords":"PyTorch, Deep Learning"}</script>
    <meta name="description" content="一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。">
<meta property="og:type" content="blog">
<meta property="og:title" content="深度學習中一些常用的內建損失函数">
<meta property="og:url" content="https:&#x2F;&#x2F;tangh.github.io&#x2F;articles&#x2F;frequently-used-built-in-loss-functions&#x2F;">
<meta property="og:site_name" content="雨天等放晴">
<meta property="og:description" content="一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-03-30T16:00:00.000Z">
<meta property="article:modified_time" content="2022-03-17T16:45:35.397Z">
<meta property="article:author" content="Tang Huan">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Loss Functions">
<meta property="article:tag" content="損失函數">
<meta property="article:tag" content="损失函数">
<meta name="twitter:card" content="summary">
    
    
        
    
    
        <meta property="og:image" content="https://tangh.github.io/assets/images/icon.jpg"/>
    
    
    
    
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+JP:wght@400;700&family=Noto+Serif+SC:wght@400;700&display=swap" rel="stylesheet">
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-iaetwm81hfopcuajcp7qnh2zsnqn4dhiu3nftuj79wdhe7fie6l4r0thrs6g.min.css">

    <!--STYLES END-->
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137837052-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-137837052-1');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    

</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            雨天等放晴
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Tang Huan</h4>
                
                    <h5 class="sidebar-profile-bio"></h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fas fa-cube" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://twitter.com/tanghrtx/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Twitter"
                        >
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.flickr.com/photos/135277712@N07/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Flickr"
                        >
                        <i class="sidebar-button-icon fab fa-flickr" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Flickr</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.instagram.com/tanghrtx/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Instagram"
                        >
                        <i class="sidebar-button-icon fab fa-instagram" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Instagram</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="YouTube"
                        >
                        <i class="sidebar-button-icon fab fa-youtube" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">YouTube</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://space.bilibili.com/634428/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="BiliBili"
                        >
                        <i class="sidebar-button-icon fab fa-youtube-square" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">BiliBili</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            深度學習中一些常用的內建損失函数
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-03-31T00:00:00+08:00">
	
		    Mar 31, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Computer-Science/">Computer Science</a>, <a class="category-link" href="/categories/Computer-Science/Deep-Learning/">Deep Learning</a>


    
</div>

    
</div>

    
    
        <div class="post-content markdown">
    
        <div class="main-content-wrap">
            <!-- excerpt -->


<p>這是最近的第二篇，目前只寫了交叉熵，以後有時間會加上別的。</p>
<h1 id="交叉熵（CrossEntropy）"><a href="#交叉熵（CrossEntropy）" class="headerlink" title="交叉熵（CrossEntropy）"></a>交叉熵（CrossEntropy）</h1><ul>
<li><code>nn.CrossEntropyLoss</code></li>
<li><code>nn.BCELoss</code></li>
<li><code>nn.BCEWithLogitsLoss</code></li>
</ul>
<h2 id="n-classes-gt-1"><a href="#n-classes-gt-1" class="headerlink" title="n_classes &gt; 1"></a>n_classes &gt; 1</h2><p>這應該是與分類相關最常見的一個損失函数，在目標檢測中對每個框的類別分数，和語義分割中對每個像素點的類別置信度訓練時，都可以使用這個損失函数。語義分割中的每一個像素就等效於目標檢測中的一個框，和圖像分類中的一整張圖。</p>
<p>在 PyTorch 中，交叉熵在 <a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss" target="_blank" rel="noopener"><code>nn.CrossEntropyLoss</code></a>，它就是將 <a href="https://pytorch.org/docs/stable/nn.html#logsoftmax" target="_blank" rel="noopener"><code>nn.LogSoftmax</code></a> 和 <a href="https://pytorch.org/docs/stable/nn.html#nllloss" target="_blank" rel="noopener"><code>nn.NLLLoss</code></a> 兩個合在一起。</p>
<p>對於 LogSoftmax，也就是在 softmax 的值上求了一个對数：</p>
<p>$$<br>\text{LogSoftmax}(x_i) = log(\frac{exp(x_i)}{\sum_j exp(x_j)})<br>$$</p>
<p>其中 <code>j</code> 是所有的類別，總數為 n_classes。</p>
<p>但是使用 LogSoftmax 比分成兩步数值上會更穩定：由於指數的存在，當輸入值 xi 過小時，在 softmax 時可能向下溢出而变為 0（在 log 之後就會是無限大）；過大時，指数計算可能會向上溢出。而計算 softmax 的 log 時可對分子分母同時除以（最）大的指数值：</p>
<p>$$<br>\begin{aligned}<br>log(softmax(x_i)) &amp; = log(\frac {\frac{e^{x_i}}{e^M}} {\frac{e^{x_1}}{e^M} + \frac{e^{x_2}}{e^M} + \cdots + \frac{e^{x_n}}{e^M}})<br>\ = \ log(\frac{e^{x_i - M}}{\sum_j e^{x_j - M}}) \\<br>&amp; = log(e^{x_i - M}) - log(\sum_j e^{x_j - M}) \\<br>&amp; = (x_i - M) - log(\sum_j e^{x_j - M})<br>\end{aligned}<br>$$</p>
<p>這樣就避免了上述問題，同時可以看到計算速度會加快不少。對輸出的每個值都（在類別通道上）完成上述計算之後，NLLLoss (negative log likelihood loss) 就是對每一個 box，取出其 GT 對應的通道上的 <code>LogSoftmax(xi)</code> 值，取相反数，最後所有取出的值相加。</p>
<p><code>class NLLLoss()</code> 在構造時有三個參数，一個是 <code>weight</code>，表示對某一個 class，取出來值取相反数後，乘上一個權重；另一個是 <code>ignore_index</code>，表示真實標籤中需要忽略的值，一般圖像中邊界和無法辨別的區域會用一個負值標記；<code>reduction=&quot;mean&quot;</code> 表示對所有 dimension 求解之後，求和並除以 <code>batch * prod(dimension)</code>，而 <code>&quot;sum&quot;</code> 表示求和後直接輸出，<code>&quot;none&quot;</code> 表示直接輸出数組 <code>shape = (batch, d1, ..., dk)</code>。</p>
<p>下面測試一下，假設是圖像分類 <code>batch = 3, n_classes = 4</code>，那麼 model output logits 應是一個 <code>(3, 4)</code> 的 tensor，而 GT 應該是 <code>(3，)</code> tensor（其他 task 只是有更多 dimension <code>N C d1 ... dk</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">prediction_logtis = torch.rand(<span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># (batch, C)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(</span><br><span class="line">    [[<span class="number">0.2870</span>, <span class="number">0.2232</span>, <span class="number">0.6423</span>, <span class="number">0.6035</span>],</span><br><span class="line">     [<span class="number">0.4180</span>, <span class="number">0.9434</span>, <span class="number">0.6034</span>, <span class="number">0.5521</span>],</span><br><span class="line">     [<span class="number">0.0026</span>, <span class="number">0.4993</span>, <span class="number">0.7166</span>, <span class="number">0.2258</span>]]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">log_sm = F.log_softmax(prediction_logtis, dim=<span class="number">-1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(</span><br><span class="line">    [[<span class="number">-1.5554</span>, <span class="number">-1.6192</span>, <span class="number">-1.2001</span>, <span class="number">-1.2389</span>],</span><br><span class="line">     [<span class="number">-1.6171</span>, <span class="number">-1.0916</span>, <span class="number">-1.4317</span>, <span class="number">-1.4830</span>],</span><br><span class="line">     [<span class="number">-1.7810</span>, <span class="number">-1.2842</span>, <span class="number">-1.0669</span>, <span class="number">-1.5578</span>]]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">math.log(math.exp(<span class="number">0.2870</span>) / (math.exp(<span class="number">0.2870</span>)+math.exp(<span class="number">0.2232</span>)+math.exp(<span class="number">0.6423</span>)+math.exp(<span class="number">0.6035</span>)))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">-1.5554288144704078</span></span><br><span class="line"></span><br><span class="line">target = torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])  <span class="comment"># (batch, )</span></span><br><span class="line">nll_loss = torch.nn.NLLLoss(reduction=<span class="string">"mean"</span>)</span><br><span class="line">nll_loss(log_sm, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.4238</span>)</span><br></pre></td></tr></table></figure>

<p>手動計算一下 loss：<code>(1.5554 + 1.4317 + 1.2842) / 3 = 1.4238</code>。</p>
<p>這種是 label 為 long int 的形式，在 tensorflow 中為 v1: <code>sparse_softmax_cross_entropy_with_logits</code> v2: <code>tf.keras.losses.SparseCategoricalCrossentropy</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf_pred = prediction_logtis.detach().numpy()</span><br><span class="line">tf_target = target.detach().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv1</span></span><br><span class="line">tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(labels=tf_target, logits=tf_pred)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.5553646</span>, <span class="number">1.4316716</span>, <span class="number">1.2842305</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv2</span></span><br><span class="line">scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">scce(tf_target, tf_pred)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4237556</span>&gt;</span><br></pre></td></tr></table></figure>

<p>此外 tf 還允許一種 one hot 形式的 label，v1: <code>softmax_cross_entropy_with_logits_v2</code> v2: <code>tf.keras.losses.CategoricalCrossentropy</code>，例如上面的 label 可以改為</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch</span></span><br><span class="line">target_onehot = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">]  <span class="comment"># (minibatch, C)</span></span><br><span class="line">nll_loss_onehot = -(torch.tensor(target_onehot) * log_sm).sum() / prediction.shape[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.4238</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv1</span></span><br><span class="line">tf_nll_loss_onehot = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(target_onehot, tf_pred)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(<span class="number">3</span>,), dtype=float32, numpy=array([<span class="number">1.5553646</span>, <span class="number">1.4316716</span>, <span class="number">1.2842305</span>], dtype=float32)&gt;</span><br><span class="line"></span><br><span class="line">tf.math.reduce_mean(tf_nll_loss_onehot)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4237556</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv2</span></span><br><span class="line">tf_cce = tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">tf_cce(target_onehot, tf_pred)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.4237556</span>&gt;</span><br></pre></td></tr></table></figure>

<p>對於 one-hot 的形式，還可以改為使用 soft label，表示其概率上不獨立，即 <code>label:0 -&gt; [1, 0, 0] -&gt; [0.9, 0.09, 0.01]</code>，tensorflow 可以直接使用，pytorch 需要自定義一個，即上述手動的乘法。我發現 <a href="https://github.com/open-mmlab/mmclassification/blob/master/mmcls/models/losses/cross_entropy_loss.py#L43" target="_blank" rel="noopener">mmcls/soft_cross_entropy</a> 有一個現成的自定義 <code>CrossEntropyLoss</code> 實現，和上面一回事。</p>
<br/>

<p>對於 <code>nn.CrossEntropyLoss</code>，如前所述，就是把兩步合在了一起，所以它的公式如下：</p>
<p>$$<br>\text{CELoss} (x, \text{class}) = - log(\frac{e^{x[class]}}{\sum_j e^{x[j]}}) = -x[class] + log(\sum_j e^{x[j]})<br>$$</p>
<p>它的參数也與 NLLLoss 相同，這裡以一個 <code>Batch Size = 2, classes = 3, size = (3, 3)</code> 的輸出為例，即語義分割任務，看看兩者是不是相等的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">prediction = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 語義分割 GT 需要 NHW，int64 格式，shape = (B, d1, d2)</span></span><br><span class="line">target = torch.tensor([</span><br><span class="line">    [[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]],</span><br><span class="line">    [[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">log_sm = F.log_softmax(prediction, dim=<span class="number">1</span>)</span><br><span class="line">nlll = nn.NLLLoss(reduction=<span class="string">"sum"</span>)</span><br><span class="line">print(nlll(log_sm, target) / <span class="number">18</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.1208</span>)</span><br><span class="line"></span><br><span class="line">cel = nn.CrossEntropyLoss(reduction=<span class="string">"mean"</span>)</span><br><span class="line">print(cel(prediction, target))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.1208</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到二者結果完全相同。</p>
<h2 id="n-class-1"><a href="#n-class-1" class="headerlink" title="n_class = 1"></a>n_class = 1</h2><p>交叉熵還有一個二元（Binary）形式的 <a href="https://pytorch.org/docs/stable/nn.html#bceloss" target="_blank" rel="noopener"><code>nn.BCELoss</code></a>，它要求 GT 只能是 <code>0</code> 或 <code>1</code> 兩個值，比如 Faster R-CNN 中的 RPN 模塊的訓練就可以使用它，只有兩類後，公式簡化如下：</p>
<p>$$<br>l_n = -w_n [y_n \cdot log(x_n) + (1-y_n) \cdot log(1-x_n) ]<br>$$</p>
<p>可以使用 <code>torch.nn.functional.binary_cross_entropy</code>，與 <code>sklearn.metrics.log_loss</code> 完全相同。</p>
<p><strong>sigmoid/softmax in binary</strong></p>
<p>The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression.</p>
<p>對於 sigmoid，最後輸出一個值，輸入是 <code>x</code>，參數是 <code>θ</code>，那麼 sigmoid 的輸出為</p>
<p>$$<br>p(y = 1|x) = \frac{1}{1 + e^{-\theta^Tx}}<br>$$</p>
<p><code>p(y = 0|x) = 1 - p(y = 1|x)</code>，所以</p>
<p>$$<br>p(y = 0|x) = \frac{e^{-\theta^Tx}}{1 + e^{-\theta^Tx}}<br>$$</p>
<p>對於 softmax，最後的輸出是兩個值，輸入是 <code>x</code>，兩個輸出的參數分別是 <code>θ_n</code>，那麼 softmax 的輸出為</p>
<p>$$<br>p(y = 1|x) = \frac{e^{\theta_1^Tx}}{e^{\theta_0^Tx} + e^{\theta_1^Tx}} = \frac{1}{1 + e^{(\theta_0^T – \theta_1^T)x}} = \frac{1}{1 + e^{-\beta x}}<br>$$<br>$$<br>p(y = 0|x) = \frac{e^{\theta_0^Tx}}{e^{\theta_0^Tx} + e^{\theta_1^Tx}} = \frac{e^{(\theta_0^T-\theta_1^T)x}}{1 + e^{(\theta_0^T-\theta_1^T)x}} = \frac{e^{-\beta x}}{1 + e^{-\beta x}}<br>$$</p>
<p>其中 \( \beta = -(\theta_0^T – \theta_1^T) \)，二分類的情況下，softmax 退化為 sigmoid。<code>nn.BCEWithLogitsLoss</code> 將 sigmoid 激活和 BCELoss 結合，同樣更數值穩定。</p>
<br/>
            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-link" href="/tags/PyTorch/" rel="tag">PyTorch</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/posts/simple-thoughts-on-chinese-characters/"
                    data-tooltip="漢字雜談"
                    aria-label="PREVIOUS: 漢字雜談"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/articles/build-model-and-dataset-in-pytorch/"
                    data-tooltip="PyTorch 中構建模型和輸入数據的方法"
                    aria-label="NEXT: PyTorch 中構建模型和輸入数據的方法"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="http://service.weibo.com/share/share.php?&amp;title=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Weibo"
                    aria-label="Share on Weibo"
                >
                    <i class="fab fa-weibo" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/&amp;title=深度學習中一些常用的內建損失函数"
                    title="Share on QQ"
                    aria-label="Share on QQ"
                >
                    <i class="fab fa-qq" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2022 Tang Huan. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-bar-actions-wrap">
    <div class="post-actions post-action-share">
        <div class="post-action">
            
                <a class="post-bar-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fas fa-angle-up" aria-hidden="true"></i>
            </a>
        </div>
        
            
                <div class="post-action">
                    <a 
                        class="post-bar-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                         <i class="fas fa-angle-down"></i>
                    </a>
                </div>
            
        
    </div>
</div>
                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="http://service.weibo.com/share/share.php?&amp;title=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Weibo"
                    >
                        <i class="fab fa-weibo" aria-hidden="true"></i><span>Share on Weibo</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="http://connect.qq.com/widget/shareqq/index.html?url=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/&amp;title=深度學習中一些常用的內建損失函数"
                        aria-label="Share on QQ"
                    >
                        <i class="fab fa-qq" aria-hidden="true"></i><span>Share on QQ</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Tang Huan</h4>
        
            <div id="about-card-bio"></div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                
            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        

<!--SCRIPTS-->

<script src="/assets/js/script-21vlobaq8sfmdbypn0z91hl6jyot6shixuux8ijser2jcbktmikbwlb6yvjx.min.js"></script>

<!--SCRIPTS END-->


    
      <script type="text/javascript">
        (function() {
          function render() {
            new Gitalk({
              clientID: 'b7b365f41dbbfaaf9b88',
              clientSecret: '25de272b8030e3c498dd56b883e4386d881b6d62',
              repo: 'tangh.github.io',
              owner: 'tangh',
              admin: ['tangh'],
              id: 'articles/frequently-used-built-in-loss-functions',
              title: document.title.replace(' - 雨天等放晴', ''),
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first","createIssueManually":true}
            }).render('gitalk');
          }
          var gc = document.createElement('script');
          gc.type = 'text/javascript';
          gc.src = '//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js';
          gc.charset = 'UTF-8';
          gc.onload = render;
          gc.async = true;
          document.querySelector('body').appendChild(gc);
          var gcs = document.createElement('link');
          gcs.href = '//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css';
          gcs.type = 'text/css';
          gcs.rel = 'stylesheet';
          gcs.media = 'screen,print';
          document.querySelector('head').appendChild(gcs);
        })();
      </script>
    




    <script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],(n=t.getBoundingClientRect()).top>=-n.height&&0<=n.left&&n.top<=1.5*(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body>
</html>
