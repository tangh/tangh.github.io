
<!DOCTYPE html>

    <html lang="zh-Hant-CN">

    
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="雨天等放晴">
    <title>深度學習中一些常用的內建損失函数 - 雨天等放晴</title>
    <meta name="author" content="Tang Huan">
    
        <meta name="keywords" content="PyTorch,Deep Learning,Loss Functions,損失函數,损失函数">
    
    
        <link rel="icon" href="https://tangh.github.io/assets/images/favicon.png">
    
    
        <link rel="apple-touch-icon" sizes="180x180" href="https://tangh.github.io/assets/images/apple-touch-icon.png">
    
    
    <script type="application/ld+json">{"@context":"http://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Tang Huan","sameAs":["https://twitter.com/tanghrtx/","https://www.flickr.com/photos/135277712@N07/","https://www.instagram.com/tanghrtx/","https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/","https://space.bilibili.com/634428/"],"image":"icon.jpg"},"articleBody":"\n\n\n這是最近的第二篇，目前只寫了交叉熵，以後有時間會加上別的。\n交叉熵（CrossEntropy）\nnn.CrossEntropyLoss\nnn.BCELoss\nnn.BCEWithLogitsLoss\n\nn_classes &gt; 1這應該是與分類相關最常見的一個損失函数，在目標檢測中對每個框的類別分数，和語義分割中對每個像素點的類別置信度訓練時，都可以使用這個損失函数。（語義分割中的每一個像素就等效於目標檢測中的一個框，和圖像分類中的一整張圖）\n在 PyTorch 中，交叉熵在 nn.CrossEntropyLoss，它就是將 nn.LogSoftmax 和 nn.NLLLoss 兩個合在一起。\n對於 LogSoftmax，也就是在 softmax 的值上求了一个對数：\n$$\\text{LogSoftmax}(x_i) = log(\\frac{exp(x_i)}{\\sum_j exp(x_j)})$$\n但是使用 LogSoftmax 比分成兩步数值上會更穩定，因為當輸入值過小時，在 softmax 後向下溢出而变為 0，那麼在 log 之後就會是無限大；當輸入值過大時，在 softmax 中的指数計算可能會向上溢出。而計算 log 時可對分子分母同時除以最大的指数值：\n$$\\begin{aligned}log[softmax(x_i)] &amp; = log(\\frac {\\frac{e^{x_i}}{e^M}} {\\frac{e^{x_1}}{e^M} + \\frac{e^{x_2}}{e^M} + \\cdots + \\frac{e^{x_n}}{e^M}})\\ = \\ log(\\frac{e^{x_i - M}}{\\sum_j e^{x_j - M}}) \\\\&amp; = log(e^{x_i - M}) - log(\\sum_j e^{x_j - M}) \\\\&amp; = (x_i - M) - log(\\sum_j e^{x_j - M})\\end{aligned}$$\n其中 j 表示的是對某一個像素（或一個框或一張圖）所有類別的概率。\n這樣就避免了上述問題，同時可以看到計算速度會加快不少。對輸出的每個值都（在類別通道上）完成上述計算之後，NLLLoss (negative log likelihood loss) 就是對每一個像素，取出這個像素 GT 對應的通道上的值，取相反数，最後所有取出的值相加。\nclass NLLLoss() 在構造時有三個參数，一個是 weight，表示對某一個類別，取出來值取相反数後，乘上一個權重；另一個是 ignore_index，表示真實標籤中需要忽略的值，一般圖像中邊界和無法辨別的區域會用一個負值標記；reduction=&quot;mean&quot; 表示對所有像素求和之後，除以整個 batch 中所有像素数量，而 &quot;sum&quot; 表示求和後直接輸出，&quot;none&quot; 表示直接輸出数組。\n下面測試一下，假設是目標檢測，共出了 3 對匹配框（整個 batch），總共有 4 類，那麼 cls 分数應是一個 (3, 4) 的 tensor，而 GT 應該是 (3，) tensor：\n123456789101112131415161718prediction = torch.rand(1, 3, 4)  # (minibatch,C,d1)&gt;&gt;&gt; tensor([        [[0.4525, 0.4324, 0.7722, 0.3753],        [0.8501, 0.8088, 0.6246, 0.3311],        [0.9967, 0.4877, 0.6705, 0.3956]]    ])target = torch.tensor([ [0, 2, 1, 0] ])  # (minibatch,d1)log_sm = F.log_softmax(prediction, dim=1)&gt;&gt;&gt; tensor([        [[-1.4379, -1.2567, -1.0174, -1.0910],        [-1.0402, -0.8803, -1.1650, -1.1352],        [-0.8936, -1.2014, -1.1192, -1.0707]]    ])nlll = torch.nn.NLLLoss(reduction=\"mean\")nlll(log_sm, target)&gt;&gt;&gt; tensor(1.2238)\n\n手動計算一下 loss：(1.4379 + 1.2014 + 1.1650 + 1.0910) / 4 = 1.223825。\n這種是 label 為 long int 的形式，在 tensorflow v2 中為 tf.keras.losses.SparseCategoricalCrossentropy，此外 tf 還允許一種 one hot 形式的 label：tf.keras.losses.CategoricalCrossentropy，例如上面的 label 可以改為\n123456789101112131415161718192021# torchtarget = [    [[1, 0, 0, 1],     [0, 0, 1, 0],     [0, 1, 0, 0]],]  # (minibatch,C=one-hot,d1)nll_loss = -(torch.tensor(target) * log_sm).sum()nll_loss / torch.prod(torch.tensor(prediction.shape)) * prediction.shape[1]&gt;&gt;&gt; tensor(1.2238)# tfv1tf_pred = prediction.detach().numpy()tf_ce = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(target, tf_pred, axis=1)&gt;&gt;&gt; &lt;tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1.4378707, 1.201431 , 1.1649961, 1.0909945]], dtype=float32)&gt;tf.math.reduce_mean(tf_nll_loss)&gt;&gt;&gt; &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.2238231&gt;# tfv2tf_ce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)tf_ce(np.swapaxes(target,1,2),np.swapaxes(tf_pred,1,2))  # channel last in tf&gt;&gt;&gt; &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.223823070526123&gt;\n\n對於 one-hot 的形式，還可以改為使用 soft label，即 label:0 -&gt; [1, 0, 0] -&gt; [0.9, 0.09, 0.01]，tensorflow 可以直接使用，pytorch 需要自定義一個。我發現 mmcls/soft_cross_entropy 有一個現成的實現，和上面一回事。\n對於 nn.CrossEntropyLoss，如前所述，就是把兩步合在了一起，所以它的公式如下：\n$$\\text{CELoss} (x, \\text{class}) = - log(\\frac{e^{x[class]}}{\\sum_j e^{x[j]}}) = -x[class] + log(\\sum_j e^{x[j]})$$\n它的參数也與 NLLLoss 相同，這裡以一個 Batch Size = 2, classes = 3, size = (3, 3) 的輸出為例，即語義分割任務，看看兩者是不是相等的：\n123456789101112131415prediction = torch.rand(2, 3, 3, 3)# 語義分割 GT 需要 NHW，int64 格式target = torch.tensor([    [[0,2,1],[0,2,1],[0,2,1]],    [[0,2,1],[0,2,1],[0,2,1]]])log_sm = F.log_softmax(prediction, dim=1)nlll = nn.NLLLoss(reduction=\"sum\")print(nlll(log_sm, target) / 18)&gt;&gt;&gt; tensor(1.1208)cel = nn.CrossEntropyLoss(reduction=\"mean\")print(cel(prediction, target))&gt;&gt;&gt; tensor(1.1208)\n\n可以看到二者結果完全相同。\nn_class = 1交叉熵還有一個二元（Binary）形式的 nn.BCELoss，它要求 GT 只能是 0 或 1 兩個值，比如 Faster R-CNN 中的 RPN 模塊的訓練就可以使用它，只有兩類後，公式簡化如下：\n$$l_n = -w_n [y_n \\cdot log(x_n) + (1-y_n) \\cdot log(1-x_n) ]$$\n可以使用 torch.nn.functional.binary_cross_entropy，與 sklearn.metrics.log_loss 完全相同。\nsigmoid/softmax in binary\nThe sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression.\n對於 sigmoid，最後輸出一個值，輸入是 x，參數是 θ，那麼 sigmoid 的輸出為\n$$p(y = 1|x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\np(y = 0|x) = 1 - p(y = 1|x)，所以\n$$p(y = 0|x) = \\frac{e^{-\\theta^Tx}}{1 + e^{-\\theta^Tx}}$$\n對於 softmax，最後的輸出是兩個值，輸入是 x，兩個輸出的參數分別是 θ_n，那麼 softmax 的輸出為\n$$p(y = 1|x) = \\frac{e^{\\theta_1^Tx}}{e^{\\theta_0^Tx} + e^{\\theta_1^Tx}} = \\frac{1}{1 + e^{(\\theta_0^T – \\theta_1^T)x}} = \\frac{1}{1 + e^{-\\beta x}}$$$$p(y = 0|x) = \\frac{e^{\\theta_0^Tx}}{e^{\\theta_0^Tx} + e^{\\theta_1^Tx}} = \\frac{e^{(\\theta_0^T-\\theta_1^T)x}}{1 + e^{(\\theta_0^T-\\theta_1^T)x}} = \\frac{e^{-\\beta x}}{1 + e^{-\\beta x}}$$\n其中 \\( \\beta = -(\\theta_0^T – \\theta_1^T) \\)，二分類的情況下，softmax 退化為 sigmoid。nn.BCEWithLogitsLoss 將 sigmoid 激活和 BCELoss 結合，同樣更數值穩定。\n","dateCreated":"2020-03-31T00:00:00+08:00","dateModified":"2021-09-11T17:34:51+08:00","datePublished":"2020-03-31T00:00:00+08:00","description":"一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。","headline":"深度學習中一些常用的內建損失函数","image":[],"mainEntityOfPage":{"@type":"WebPage","@id":"https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"},"publisher":{"@type":"Organization","name":"Tang Huan","sameAs":["https://twitter.com/tanghrtx/","https://www.flickr.com/photos/135277712@N07/","https://www.instagram.com/tanghrtx/","https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/","https://space.bilibili.com/634428/"],"image":"icon.jpg","logo":{"@type":"ImageObject","url":"icon.jpg"}},"url":"https://tangh.github.io/articles/frequently-used-built-in-loss-functions/","keywords":"PyTorch, Deep Learning"}</script>
    <meta name="description" content="一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。">
<meta property="og:type" content="blog">
<meta property="og:title" content="深度學習中一些常用的內建損失函数">
<meta property="og:url" content="https:&#x2F;&#x2F;tangh.github.io&#x2F;articles&#x2F;frequently-used-built-in-loss-functions&#x2F;">
<meta property="og:site_name" content="雨天等放晴">
<meta property="og:description" content="一些深度學習中常用的損失函数，以及它們在 PyTorch (TensorFlow) 中的使用和行為。">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-03-30T16:00:00.000Z">
<meta property="article:modified_time" content="2021-09-11T09:34:51.127Z">
<meta property="article:author" content="Tang Huan">
<meta property="article:tag" content="PyTorch">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Loss Functions">
<meta property="article:tag" content="損失函數">
<meta property="article:tag" content="损失函数">
<meta name="twitter:card" content="summary">
    
    
        
    
    
        <meta property="og:image" content="https://tangh.github.io/assets/images/icon.jpg"/>
    
    
    
    
    <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+JP:wght@400;700&family=Noto+Serif+SC:wght@400;700&display=swap" rel="stylesheet">
    <!--STYLES-->
    
<link rel="stylesheet" href="/assets/css/style-iaetwm81hfopcuajcp7qnh2zsnqn4dhiu3nftuj79wdhe7fie6l4r0thrs6g.min.css">

    <!--STYLES END-->
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-137837052-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-137837052-1');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


    

</head>

    <body>
        <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="blog">
            <!-- Define author's picture -->


    
        
            
        
    

<header id="header" data-behavior="4">
    <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
    <div class="header-title">
        <a
            class="header-title-link"
            href="/"
            aria-label=""
        >
            雨天等放晴
        </a>
    </div>
    
        
            <a
                class="header-right-picture "
                href="#about"
                aria-label="Open the link: /#about"
            >
        
        
            <img class="header-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
        
        </a>
    
</header>

            <!-- Define author's picture -->



        
    

<nav id="sidebar" data-behavior="4">
    <div class="sidebar-container">
        
            <div class="sidebar-profile">
                <a
                    href="/#about"
                    aria-label="Read more about the author"
                >
                    <img class="sidebar-profile-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
                </a>
                <h4 class="sidebar-profile-name">Tang Huan</h4>
                
                    <h5 class="sidebar-profile-bio"></h5>
                
            </div>
        
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/"
                            
                            rel="noopener"
                            title="Home"
                        >
                        <i class="sidebar-button-icon fa fa-home" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Home</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-categories"
                            
                            rel="noopener"
                            title="Categories"
                        >
                        <i class="sidebar-button-icon fa fa-bookmark" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Categories</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-tags"
                            
                            rel="noopener"
                            title="Tags"
                        >
                        <i class="sidebar-button-icon fa fa-tags" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Tags</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/all-archives"
                            
                            rel="noopener"
                            title="Archives"
                        >
                        <i class="sidebar-button-icon fa fa-archive" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Archives</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="/about"
                            
                            rel="noopener"
                            title="About"
                        >
                        <i class="sidebar-button-icon fas fa-cube" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">About</span>
                    </a>
            </li>
            
        </ul>
        
            <ul class="sidebar-buttons">
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://twitter.com/tanghrtx/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Twitter"
                        >
                        <i class="sidebar-button-icon fab fa-twitter" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Twitter</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.flickr.com/photos/135277712@N07/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Flickr"
                        >
                        <i class="sidebar-button-icon fab fa-flickr" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Flickr</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.instagram.com/tanghrtx/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="Instagram"
                        >
                        <i class="sidebar-button-icon fab fa-instagram" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">Instagram</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://www.youtube.com/channel/UCO-I0MZR6-HYmI_tgbBc0yw/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="YouTube"
                        >
                        <i class="sidebar-button-icon fab fa-youtube" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">YouTube</span>
                    </a>
            </li>
            
                <li class="sidebar-button">
                    
                        <a
                            class="sidebar-button-link "
                            href="https://space.bilibili.com/634428/"
                            
                                target="_blank"
                            
                            rel="noopener"
                            title="BiliBili"
                        >
                        <i class="sidebar-button-icon fab fa-youtube-square" aria-hidden="true"></i>
                        <span class="sidebar-button-desc">BiliBili</span>
                    </a>
            </li>
            
        </ul>
        
    </div>
</nav>

            
            <div id="main" data-behavior="4"
                 class="
                        hasCoverMetaIn
                        ">
                
<article class="post">
    
    
        <div class="post-header main-content-wrap text-center">
    
        <h1 class="post-title">
            深度學習中一些常用的內建損失函数
        </h1>
    
    
        <div class="post-meta">
    <time datetime="2020-03-31T00:00:00+08:00">
	
		    Mar 31, 2020
    	
    </time>
    
        <span>in </span>
        
    <a class="category-link" href="/categories/Computer-Science/">Computer Science</a>, <a class="category-link" href="/categories/Computer-Science/Deep-Learning/">Deep Learning</a>


    
</div>

    
</div>

    
    
        <div class="post-content markdown">
    
        <div class="main-content-wrap">
            <!-- excerpt -->


<p>這是最近的第二篇，目前只寫了交叉熵，以後有時間會加上別的。</p>
<h1 id="交叉熵（CrossEntropy）"><a href="#交叉熵（CrossEntropy）" class="headerlink" title="交叉熵（CrossEntropy）"></a>交叉熵（CrossEntropy）</h1><ul>
<li><code>nn.CrossEntropyLoss</code></li>
<li><code>nn.BCELoss</code></li>
<li><code>nn.BCEWithLogitsLoss</code></li>
</ul>
<h2 id="n-classes-gt-1"><a href="#n-classes-gt-1" class="headerlink" title="n_classes &gt; 1"></a>n_classes &gt; 1</h2><p>這應該是與分類相關最常見的一個損失函数，在目標檢測中對每個框的類別分数，和語義分割中對每個像素點的類別置信度訓練時，都可以使用這個損失函数。（語義分割中的每一個像素就等效於目標檢測中的一個框，和圖像分類中的一整張圖）</p>
<p>在 PyTorch 中，交叉熵在 <a href="https://pytorch.org/docs/stable/nn.html#crossentropyloss" target="_blank" rel="noopener"><code>nn.CrossEntropyLoss</code></a>，它就是將 <a href="https://pytorch.org/docs/stable/nn.html#logsoftmax" target="_blank" rel="noopener"><code>nn.LogSoftmax</code></a> 和 <a href="https://pytorch.org/docs/stable/nn.html#nllloss" target="_blank" rel="noopener"><code>nn.NLLLoss</code></a> 兩個合在一起。</p>
<p>對於 LogSoftmax，也就是在 softmax 的值上求了一个對数：</p>
<p>$$<br>\text{LogSoftmax}(x_i) = log(\frac{exp(x_i)}{\sum_j exp(x_j)})<br>$$</p>
<p>但是使用 LogSoftmax 比分成兩步数值上會更穩定，因為當輸入值過小時，在 softmax 後向下溢出而变為 0，那麼在 log 之後就會是無限大；當輸入值過大時，在 softmax 中的指数計算可能會向上溢出。而計算 log 時可對分子分母同時除以最大的指数值：</p>
<p>$$<br>\begin{aligned}<br>log[softmax(x_i)] &amp; = log(\frac {\frac{e^{x_i}}{e^M}} {\frac{e^{x_1}}{e^M} + \frac{e^{x_2}}{e^M} + \cdots + \frac{e^{x_n}}{e^M}})<br>\ = \ log(\frac{e^{x_i - M}}{\sum_j e^{x_j - M}}) \\<br>&amp; = log(e^{x_i - M}) - log(\sum_j e^{x_j - M}) \\<br>&amp; = (x_i - M) - log(\sum_j e^{x_j - M})<br>\end{aligned}<br>$$</p>
<p>其中 <code>j</code> 表示的是對某一個像素（或一個框或一張圖）所有類別的概率。</p>
<p>這樣就避免了上述問題，同時可以看到計算速度會加快不少。對輸出的每個值都（在類別通道上）完成上述計算之後，NLLLoss (negative log likelihood loss) 就是對每一個像素，取出這個像素 GT 對應的通道上的值，取相反数，最後所有取出的值相加。</p>
<p><code>class NLLLoss()</code> 在構造時有三個參数，一個是 <code>weight</code>，表示對某一個類別，取出來值取相反数後，乘上一個權重；另一個是 <code>ignore_index</code>，表示真實標籤中需要忽略的值，一般圖像中邊界和無法辨別的區域會用一個負值標記；<code>reduction=&quot;mean&quot;</code> 表示對所有像素求和之後，除以整個 batch 中所有像素数量，而 <code>&quot;sum&quot;</code> 表示求和後直接輸出，<code>&quot;none&quot;</code> 表示直接輸出数組。</p>
<p>下面測試一下，假設是目標檢測，共出了 3 對匹配框（整個 batch），總共有 4 類，那麼 cls 分数應是一個 <code>(3, 4)</code> 的 tensor，而 GT 應該是 <code>(3，)</code> tensor：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">prediction = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># (minibatch,C,d1)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([</span><br><span class="line">        [[<span class="number">0.4525</span>, <span class="number">0.4324</span>, <span class="number">0.7722</span>, <span class="number">0.3753</span>],</span><br><span class="line">        [<span class="number">0.8501</span>, <span class="number">0.8088</span>, <span class="number">0.6246</span>, <span class="number">0.3311</span>],</span><br><span class="line">        [<span class="number">0.9967</span>, <span class="number">0.4877</span>, <span class="number">0.6705</span>, <span class="number">0.3956</span>]]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">target = torch.tensor([ [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>] ])  <span class="comment"># (minibatch,d1)</span></span><br><span class="line">log_sm = F.log_softmax(prediction, dim=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor([</span><br><span class="line">        [[<span class="number">-1.4379</span>, <span class="number">-1.2567</span>, <span class="number">-1.0174</span>, <span class="number">-1.0910</span>],</span><br><span class="line">        [<span class="number">-1.0402</span>, <span class="number">-0.8803</span>, <span class="number">-1.1650</span>, <span class="number">-1.1352</span>],</span><br><span class="line">        [<span class="number">-0.8936</span>, <span class="number">-1.2014</span>, <span class="number">-1.1192</span>, <span class="number">-1.0707</span>]]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">nlll = torch.nn.NLLLoss(reduction=<span class="string">"mean"</span>)</span><br><span class="line">nlll(log_sm, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.2238</span>)</span><br></pre></td></tr></table></figure>

<p>手動計算一下 loss：<code>(1.4379 + 1.2014 + 1.1650 + 1.0910) / 4 = 1.223825</code>。</p>
<p>這種是 label 為 long int 的形式，在 tensorflow v2 中為 <code>tf.keras.losses.SparseCategoricalCrossentropy</code>，此外 tf 還允許一種 one hot 形式的 label：<code>tf.keras.losses.CategoricalCrossentropy</code>，例如上面的 label 可以改為</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch</span></span><br><span class="line">target = [</span><br><span class="line">    [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">     [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">]  <span class="comment"># (minibatch,C=one-hot,d1)</span></span><br><span class="line">nll_loss = -(torch.tensor(target) * log_sm).sum()</span><br><span class="line">nll_loss / torch.prod(torch.tensor(prediction.shape)) * prediction.shape[<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.2238</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv1</span></span><br><span class="line">tf_pred = prediction.detach().numpy()</span><br><span class="line">tf_ce = tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(target, tf_pred, axis=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(<span class="number">1</span>, <span class="number">4</span>), dtype=float32, numpy=array([[<span class="number">1.4378707</span>, <span class="number">1.201431</span> , <span class="number">1.1649961</span>, <span class="number">1.0909945</span>]], dtype=float32)&gt;</span><br><span class="line">tf.math.reduce_mean(tf_nll_loss)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="number">1.2238231</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># tfv2</span></span><br><span class="line">tf_ce = tf.keras.losses.CategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line">tf_ce(np.swapaxes(target,<span class="number">1</span>,<span class="number">2</span>),np.swapaxes(tf_pred,<span class="number">1</span>,<span class="number">2</span>))  <span class="comment"># channel last in tf</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>&lt;tf.Tensor: shape=(), dtype=float64, numpy=<span class="number">1.223823070526123</span>&gt;</span><br></pre></td></tr></table></figure>

<p>對於 one-hot 的形式，還可以改為使用 soft label，即 <code>label:0 -&gt; [1, 0, 0] -&gt; [0.9, 0.09, 0.01]</code>，tensorflow 可以直接使用，pytorch 需要自定義一個。我發現 <a href="https://github.com/open-mmlab/mmclassification/blob/master/mmcls/models/losses/cross_entropy_loss.py#L35" target="_blank" rel="noopener">mmcls/soft_cross_entropy</a> 有一個現成的實現，和上面一回事。</p>
<p>對於 <code>nn.CrossEntropyLoss</code>，如前所述，就是把兩步合在了一起，所以它的公式如下：</p>
<p>$$<br>\text{CELoss} (x, \text{class}) = - log(\frac{e^{x[class]}}{\sum_j e^{x[j]}}) = -x[class] + log(\sum_j e^{x[j]})<br>$$</p>
<p>它的參数也與 NLLLoss 相同，這裡以一個 <code>Batch Size = 2, classes = 3, size = (3, 3)</code> 的輸出為例，即語義分割任務，看看兩者是不是相等的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">prediction = torch.rand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 語義分割 GT 需要 NHW，int64 格式</span></span><br><span class="line">target = torch.tensor([</span><br><span class="line">    [[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]],</span><br><span class="line">    [[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">log_sm = F.log_softmax(prediction, dim=<span class="number">1</span>)</span><br><span class="line">nlll = nn.NLLLoss(reduction=<span class="string">"sum"</span>)</span><br><span class="line">print(nlll(log_sm, target) / <span class="number">18</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.1208</span>)</span><br><span class="line"></span><br><span class="line">cel = nn.CrossEntropyLoss(reduction=<span class="string">"mean"</span>)</span><br><span class="line">print(cel(prediction, target))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor(<span class="number">1.1208</span>)</span><br></pre></td></tr></table></figure>

<p>可以看到二者結果完全相同。</p>
<h2 id="n-class-1"><a href="#n-class-1" class="headerlink" title="n_class = 1"></a>n_class = 1</h2><p>交叉熵還有一個二元（Binary）形式的 <a href="https://pytorch.org/docs/stable/nn.html#bceloss" target="_blank" rel="noopener"><code>nn.BCELoss</code></a>，它要求 GT 只能是 <code>0</code> 或 <code>1</code> 兩個值，比如 Faster R-CNN 中的 RPN 模塊的訓練就可以使用它，只有兩類後，公式簡化如下：</p>
<p>$$<br>l_n = -w_n [y_n \cdot log(x_n) + (1-y_n) \cdot log(1-x_n) ]<br>$$</p>
<p>可以使用 <code>torch.nn.functional.binary_cross_entropy</code>，與 <code>sklearn.metrics.log_loss</code> 完全相同。</p>
<p><strong>sigmoid/softmax in binary</strong></p>
<p>The sigmoid function is used for the two-class logistic regression, whereas the softmax function is used for the multiclass logistic regression.</p>
<p>對於 sigmoid，最後輸出一個值，輸入是 <code>x</code>，參數是 <code>θ</code>，那麼 sigmoid 的輸出為</p>
<p>$$<br>p(y = 1|x) = \frac{1}{1 + e^{-\theta^Tx}}<br>$$</p>
<p><code>p(y = 0|x) = 1 - p(y = 1|x)</code>，所以</p>
<p>$$<br>p(y = 0|x) = \frac{e^{-\theta^Tx}}{1 + e^{-\theta^Tx}}<br>$$</p>
<p>對於 softmax，最後的輸出是兩個值，輸入是 <code>x</code>，兩個輸出的參數分別是 <code>θ_n</code>，那麼 softmax 的輸出為</p>
<p>$$<br>p(y = 1|x) = \frac{e^{\theta_1^Tx}}{e^{\theta_0^Tx} + e^{\theta_1^Tx}} = \frac{1}{1 + e^{(\theta_0^T – \theta_1^T)x}} = \frac{1}{1 + e^{-\beta x}}<br>$$<br>$$<br>p(y = 0|x) = \frac{e^{\theta_0^Tx}}{e^{\theta_0^Tx} + e^{\theta_1^Tx}} = \frac{e^{(\theta_0^T-\theta_1^T)x}}{1 + e^{(\theta_0^T-\theta_1^T)x}} = \frac{e^{-\beta x}}{1 + e^{-\beta x}}<br>$$</p>
<p>其中 \( \beta = -(\theta_0^T – \theta_1^T) \)，二分類的情況下，softmax 退化為 sigmoid。<code>nn.BCEWithLogitsLoss</code> 將 sigmoid 激活和 BCELoss 結合，同樣更數值穩定。</p>
<br/>
            


        </div>
    </div>
    <div id="post-footer" class="post-footer main-content-wrap">
        
            <div class="post-footer-tags">
                <span class="text-color-light text-small">TAGGED IN</span><br/>
                
    <a class="tag tag--primary tag--small t-link" href="/tags/Deep-Learning/" rel="tag">Deep Learning</a> <a class="tag tag--primary tag--small t-link" href="/tags/PyTorch/" rel="tag">PyTorch</a>

            </div>
        
        
            <div class="post-actions-wrap">
    <nav>
        <ul class="post-actions post-action-nav">
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/posts/simple-thoughts-on-chinese-characters/"
                    data-tooltip="漢字雜談"
                    aria-label="PREVIOUS: 漢字雜談"
                >
                    
                        <i class="fa fa-angle-left" aria-hidden="true"></i>
                        <span class="hide-xs hide-sm text-small icon-ml">PREVIOUS</span>
                    </a>
            </li>
            <li class="post-action">
                
                    
                <a
                    class="post-action-btn btn btn--default tooltip--top"
                    href="/articles/build-model-and-dataset-in-pytorch/"
                    data-tooltip="PyTorch 中構建模型和輸入数據的方法"
                    aria-label="NEXT: PyTorch 中構建模型和輸入数據的方法"
                >
                    
                        <span class="hide-xs hide-sm text-small icon-mr">NEXT</span>
                        <i class="fa fa-angle-right" aria-hidden="true"></i>
                    </a>
            </li>
        </ul>
    </nav>
    <ul class="post-actions post-action-share">
        <li class="post-action hide-lg hide-md hide-sm">
            <a
                class="post-action-btn btn btn--default btn-open-shareoptions"
                href="#btn-open-shareoptions"
                aria-label="Share this post"
            >
                <i class="fa fa-share-alt" aria-hidden="true"></i>
            </a>
        </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://www.facebook.com/sharer/sharer.php?u=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Facebook"
                    aria-label="Share on Facebook"
                >
                    <i class="fab fa-facebook" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="https://twitter.com/intent/tweet?text=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Twitter"
                    aria-label="Share on Twitter"
                >
                    <i class="fab fa-twitter" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="http://service.weibo.com/share/share.php?&amp;title=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                    title="Share on Weibo"
                    aria-label="Share on Weibo"
                >
                    <i class="fab fa-weibo" aria-hidden="true"></i>
                </a>
            </li>
        
            
            
            <li class="post-action hide-xs">
                <a
                    class="post-action-btn btn btn--default"
                    target="new" href="http://connect.qq.com/widget/shareqq/index.html?url=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/&amp;title=深度學習中一些常用的內建損失函数"
                    title="Share on QQ"
                    aria-label="Share on QQ"
                >
                    <i class="fab fa-qq" aria-hidden="true"></i>
                </a>
            </li>
        
        
            
                <li class="post-action">
                    <a
                        class="post-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                        <i class="fa fa-comment"></i>
                    </a>
                </li>
            
        
        <li class="post-action">
            
                <a class="post-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fa fa-list" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>


        
        
            
                <div id="gitalk"></div>

            
        
    </div>
</article>



                <footer id="footer" class="main-content-wrap">
    <span class="copyrights">
        Copyrights &copy; 2021 Tang Huan. All Rights Reserved.
    </span>
</footer>

            </div>
            
                <div id="bottom-bar" class="post-bottom-bar" data-behavior="4">
                    <div class="post-bar-actions-wrap">
    <div class="post-actions post-action-share">
        <div class="post-action">
            
                <a class="post-bar-action-btn btn btn--default" href="#" aria-label="Back to top">
            
                <i class="fas fa-angle-up" aria-hidden="true"></i>
            </a>
        </div>
        
            
                <div class="post-action">
                    <a 
                        class="post-bar-action-btn btn btn--default"
                        href="#gitalk"
                        aria-label="Leave a comment"
                    >
                         <i class="fas fa-angle-down"></i>
                    </a>
                </div>
            
        
    </div>
</div>
                </div>
                
    <div id="share-options-bar" class="share-options-bar" data-behavior="4">
        <i id="btn-close-shareoptions" class="fa fa-times"></i>
        <ul class="share-options">
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://www.facebook.com/sharer/sharer.php?u=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Facebook"
                    >
                        <i class="fab fa-facebook" aria-hidden="true"></i><span>Share on Facebook</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="https://twitter.com/intent/tweet?text=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Twitter"
                    >
                        <i class="fab fa-twitter" aria-hidden="true"></i><span>Share on Twitter</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="http://service.weibo.com/share/share.php?&amp;title=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/"
                        aria-label="Share on Weibo"
                    >
                        <i class="fab fa-weibo" aria-hidden="true"></i><span>Share on Weibo</span>
                    </a>
                </li>
            
                
                
                <li class="share-option">
                    <a
                        class="share-option-btn"
                        target="new"
                        href="http://connect.qq.com/widget/shareqq/index.html?url=https://tangh.github.io/articles/frequently-used-built-in-loss-functions/&amp;title=深度學習中一些常用的內建損失函数"
                        aria-label="Share on QQ"
                    >
                        <i class="fab fa-qq" aria-hidden="true"></i><span>Share on QQ</span>
                    </a>
                </li>
            
        </ul>
    </div>


            
        </div>
        


    
        
    

<div id="about">
    <div id="about-card">
        <div id="about-btn-close">
            <i class="fa fa-times"></i>
        </div>
        
            <img id="about-card-picture" src="/assets/images/icon.jpg" alt="Author&#39;s picture"/>
        
            <h4 id="about-card-name">Tang Huan</h4>
        
            <div id="about-card-bio"></div>
        
        
            <div id="about-card-job">
                <i class="fa fa-briefcase"></i>
                <br/>
                
            </div>
        
        
            <div id="about-card-location">
                <i class="fa fa-map-marker-alt"></i>
                <br/>
                Shanghai
            </div>
        
    </div>
</div>

        
        
<div id="cover" style="background-image:url('/assets/images/cover.jpg');"></div>
        

<!--SCRIPTS-->

<script src="/assets/js/script-21vlobaq8sfmdbypn0z91hl6jyot6shixuux8ijser2jcbktmikbwlb6yvjx.min.js"></script>

<!--SCRIPTS END-->


    
      <script type="text/javascript">
        (function() {
          function render() {
            new Gitalk({
              clientID: 'b7b365f41dbbfaaf9b88',
              clientSecret: '25de272b8030e3c498dd56b883e4386d881b6d62',
              repo: 'tangh.github.io',
              owner: 'tangh',
              admin: ['tangh'],
              id: 'articles/frequently-used-built-in-loss-functions',
              title: document.title.replace(' - 雨天等放晴', ''),
              ...{"language":"en","perPage":10,"distractionFreeMode":false,"enableHotKey":true,"pagerDirection":"first","createIssueManually":true}
            }).render('gitalk');
          }
          var gc = document.createElement('script');
          gc.type = 'text/javascript';
          gc.src = '//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js';
          gc.charset = 'UTF-8';
          gc.onload = render;
          gc.async = true;
          document.querySelector('body').appendChild(gc);
          var gcs = document.createElement('link');
          gcs.href = '//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css';
          gcs.type = 'text/css';
          gcs.rel = 'stylesheet';
          gcs.media = 'screen,print';
          document.querySelector('head').appendChild(gcs);
        })();
      </script>
    




    <script>!function(e){var c=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function i(){for(var r=0;r<c.length;r++)t=c[r],(n=t.getBoundingClientRect()).top>=-n.height&&0<=n.left&&n.top<=1.5*(e.innerHeight||document.documentElement.clientHeight)&&function(){var t,n,e,i,o=c[r];t=o,n=function(){c=c.filter(function(t){return o!==t})},e=new Image,i=t.getAttribute("data-original"),e.onload=function(){t.src=i,n&&n()},e.src=i}();var t,n}i(),e.addEventListener("scroll",function(){var t,n;t=i,n=e,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(n)},500)})}(this);</script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body>
</html>
